{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4 NLP with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 1. Representing text as Tensors\n",
    "\n",
    "If we want to solve Natural Language Processing (NLP) tasks with neural networks, we need some way to represent text as tensors. Computers already represent textual characters as numbers that map to fonts on your screen using encodings such as ASCII or UTF-8.\n",
    "\n",
    "For example, when you type \"Hello\", The computer will see [1001000,1100101,etc] Where H-> 1001000, e-> 1100101.\n",
    "\n",
    "Human understand what each letter represents, and how all characters come together to form the words of a sentence. However, computers by themselves do not have such an understanding, and neural network has to learn the meaning of words during training.\n",
    "\n",
    "Therefore, we can use different approaches when representing text:\n",
    "\n",
    "1. Character-level representation, when we represent text by treating each character as a number. Given that we have 'C' different characters in our text corpus, the word 'Hello' would be represented by '5Ã—C' tensor. Each letter would correspond to a tensor column in one-hot encoding.\n",
    "\n",
    "2. Word-level representation, in which we create a vocabulary of all words in our text, and then represent words using one-hot encoding. This approach is somehow better, because each letter by itself does not have much meaning, and thus by using higher-level semantic concepts - words - we simplify the task for the neural network. However, given large dictionary size, we need to deal with high-dimensional sparse tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Text classification task\n",
    "In this module, we will start with a simple text classification task based on AG_NEWS dataset, which is to classify news headlines into one of 4 categories: \n",
    "- World, \n",
    "- Sports, \n",
    "- Business\n",
    "- Sci/Tech. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Download data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "from collections import Counter, OrderedDict\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "# This dataset is built into torchtext module, so we can easily access it by using torchtext.datasets.\n",
    "path = \"/tmp/pytorch/data\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "# torchtext.datasets returns iterators\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root=path)\n",
    "# to reuse data, we convert iterators to list\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Explore the data set\n",
    "\n",
    "The train_dataset and test_dataset contain iterators of rows. Each row has two columns:\n",
    "- label (number of class, e.g. 0->World, 1->Sports, 2->Business, 3->Sci/Tech)\n",
    "- text\n",
    "\n",
    "Below is an example of a row, 3 is the label(business), the string is the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 3 -> text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "label: 3 -> text: Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "label: 3 -> text: Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "label: 3 -> text: Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "label: 3 -> text: Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "# print 5 first rows of the data set\n",
    "for i, x in zip(range(5), train_dataset):\n",
    "    print(f\"label: {x[0]} -> text: {x[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Transform text to tensors\n",
    "\n",
    "To make text readable by Neuron Network, we need to convert text into tensors. \n",
    "\n",
    "#### 4.2.3.1\n",
    "First step: we convert text into numbers. And we want word-level representation, we need to do two things:\n",
    "\n",
    "- use tokenizer to split text into tokens\n",
    "- build a vocabulary of those tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch provide basic tokenizer, here is an example\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use counter to store the generated token to take in account the token frequency\n",
    "counter = Counter()\n",
    "# we iterate over all rows, covert text to word token, and add these token to bag_of words\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "# sort the token counter by token's frequencies\n",
    "sorted_by_token_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "# build a set of words as an orderedDict\n",
    "words_dict = OrderedDict(sorted_by_token_freq_tuples)\n",
    "# we build a vocabulary based on the words token\n",
    "vocab1 = vocab(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95810\n"
     ]
    }
   ],
   "source": [
    "# check the size of the vacabulary\n",
    "vocab_size = len(vocab1)\n",
    "print(f\"Vocab size if {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[281, 2318, 3, 335, 17, 1299, 2353]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can easily convert a text to a set of numbers by using the generated vocabulary\n",
    "def encode(vocabulary, text):\n",
    "    return [vocabulary[word] for word in tokenizer(text)]\n",
    "\n",
    "\n",
    "encode(vocab1, 'I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3.2 Bag of words text representation\n",
    "\n",
    "In step1, we have converted texts to numbers, now we want to convert these numbers to tensors. Bag of words is one of the ways to do so.\n",
    "\n",
    "Because words represent meaning, sometimes we can figure out the meaning of a text by just looking at the individual words, regardless of their order in the sentence. For example, when classifying news, words like weather, snow are likely to indicate weather forecast, while words like stocks, dollar would count towards financial news.\n",
    "\n",
    "Bag of Words (BoW) vector representation is the most commonly used traditional vector representation. Each word is linked to a vector index, vector element contains the number of occurrences of a word in a given document.\n",
    "\n",
    "Note: You can also think of BoW as a sum of all one-hot-encoded vectors for individual words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a bag of words by using scikit learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    'I like hot dogs.',\n",
    "    'The dog ran fast.',\n",
    "    'Its hot outside.',\n",
    "]\n",
    "# train the vectorizer with above text\n",
    "vectorizer.fit_transform(corpus)\n",
    "\n",
    "# use the trained vectorizer to transform a text\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2: convert encoded text to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary argument is the vocabulary of all the token generated from the dataset\n",
    "# text argument is the input text that you want to transform\n",
    "# bow_vocab_size specify the default size of the bow vocabulary.\n",
    "def to_bow(vocabulary, text, bow_vocab_size):\n",
    "    # create a one dimension tensor that has the size of bow_vocab_size, and float type\n",
    "    result = torch.zeros(bow_vocab_size, dtype=torch.float32)\n",
    "    # encode convert text to a list of indices of the token in the vocabulary\n",
    "    for i in encode(vocabulary, text):\n",
    "        if i < bow_vocab_size:\n",
    "            result[i] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since often the vocabulary size is pretty big, we can limit the size of the vocabulary to most frequent words. Try lowering vocab_size value and running the text classifier model training code below, and see how it affects the accuracy. You should expect some accuracy drop, but not dramatic, in lieu of higher performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab1)\n",
    "\n",
    "print(to_bow(vocab1, train_dataset[0][1], vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Training BoW classifier\n",
    "Now that we have learned how to build Bag-of-Words representation of our text, let's train a classifier on top of it. \n",
    "\n",
    "#### 4.2.4.1 Prepareing data\n",
    "First, we need to convert our dataset for training in such a way, that all positional vector representations are converted to bag-of-words representation. This can be achieved by passing bowify function as collate_fn parameter to standard torch DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(news_data_batch):\n",
    "    return (\n",
    "        # for items of a batch, we convert their label digit to one tensor\n",
    "        # note each item has two elements, item[0] is the label, item[1] is the text\n",
    "        torch.LongTensor([item[0] - 1 for item in news_data_batch]),\n",
    "        torch.stack([to_bow(vocab1, item[1], vocab_size) for item in news_data_batch])\n",
    "    )\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4.2 Build nn classifier model\n",
    "\n",
    "The classifier neural network contains one linear layer. The size of the input vector equals to vocab_size, and output size corresponds to the number of the news classes which is 4 (e.g. e.g. 0->World, 1->Sports, 2->Business, 3->Sci/Tech). \n",
    "\n",
    "\n",
    "Because we are solving classification task, the final activation function is LogSoftmax()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, I did not sub class torch.nn.module to build our model\n",
    "model = torch.nn.Sequential(torch.nn.Linear(vocab_size, 4), torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4.3 Build training loop\n",
    "\n",
    "Now we will define standard PyTorch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is the nn that we want to train, \n",
    "# data is the training data in dataloader format\n",
    "# lr is the learning rate\n",
    "# loss_fn is the loss function\n",
    "# epoch_size defines how many times we want to train the model with data.\n",
    "# report_freq defines the frequency of reporting\n",
    "# optimizer defines how the model optimize its parameter of each layer with the loss computed by loss function\n",
    "\n",
    "def train_loop(model, data, lr=0.01, optimizer=None, loss_fn=torch.nn.NLLLoss(), epoch_size=None, report_freq=200):\n",
    "    # set optimizer, if nono provided use the default one\n",
    "    optimizer = optimizer or torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    # we reset total_loss, acc, count,i for each training loop to avoid cumulating the numbers\n",
    "    total_loss, acc, count, i = 0, 0, 0, 0\n",
    "    for labels, features in data:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(features)\n",
    "        loss = loss_fn(out, labels)  #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        acc += (predicted == labels).sum()\n",
    "        count += len(labels)\n",
    "        i += 1\n",
    "        if i % report_freq == 0:\n",
    "            print(f\"{count}: acc={acc.item() / count}\")\n",
    "        if epoch_size and count > epoch_size:\n",
    "            break\n",
    "    return total_loss.item() / count, acc.item() / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8021875\n",
      "6400: acc=0.83703125\n",
      "9600: acc=0.8516666666666667\n",
      "12800: acc=0.859296875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.025697248576800707, 0.8637393390191898)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loop(model, train_loader, epoch_size=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}