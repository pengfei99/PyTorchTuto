{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 Embeddings\n",
    "\n",
    "In section 1 ConvertTextToTensors, we operated on high-dimensional bag-of-words vectors with the length as the size of the vocabulary,\n",
    "and we were explicitly converting from low-dimensional positional representation vectors into sparse one-hot\n",
    "representation. This one-hot representation is not memory-efficient, in addition, each word is treated\n",
    "independently of each other, i.e. one-hot encoded vectors do not express any semantic similarity between words.\n",
    "\n",
    "## 2.1 What is embedding?\n",
    "The idea of embedding is to represent words by \"lower-dimensional dense vectors\", which somehow reflect\n",
    "semantic meaning of a word. We will later discuss how to build meaningful word embeddings, but for now\n",
    "let's just think of embeddings as a way to lower dimensionality of a word vector.\n",
    "\n",
    "So, embedding layer would take a word as an input, and produce an output vector of specified embedding_size.\n",
    "In a sense, it is very similar to Linear layer, but instead of taking one-hot encoded vector, it will be\n",
    "able to take a word number as an input.\n",
    "\n",
    "## 2.2 First model that use embedding layer as input layer\n",
    "By using embedding layer as a first layer in our network, we can switch from bag-or-words to embedding bag\n",
    "model, where we first convert each word in our text into corresponding embedding, and then compute some\n",
    "aggregate function over all those embeddings, such as sum, average or max.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter, OrderedDict\n",
    "from torchtext.vocab import vocab\n",
    "import torchtext"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ClassifierWithSameLengthEmbedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first layer is embedding\n",
    "        x = self.embedding(x)\n",
    "        # second layer calculate mean of the embedding\n",
    "        x = torch.mean(x, dim=1)\n",
    "        # third layer is linear\n",
    "        return self.fc(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Dealing with variable sequence size\n",
    "\n",
    "As a result of this architecture, mini-batches to our network would need to be created in a certain way. In section 1,\n",
    "when using bag-of-words, all BoW tensors in a mini-batch had equal size vocab_size, regardless of the actual length\n",
    "of the text sequence. Once we move to word embeddings, we would end up with variable number of words in each text\n",
    "sample, and when combining those samples into mini-batches we would have to apply some padding functions.\n",
    "\n",
    "This can be done using the collate_fn function to the datasource. In this tutorial we use two different method:\n",
    "- padding the text tensor with zero, so all tensors have the same length which is the max text length of a batch\n",
    "- using offset vector, which would hold offsets of all sequences stored in one large vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build a simple tokenizer\n",
    "my_tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# build label class list\n",
    "label_classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "\n",
    "def load_dataset(storage_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root=storage_path)\n",
    "    train_dataset = list(train_dataset)\n",
    "    test_dataset = list(test_dataset)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "path = \"/tmp/pytorch/data\"\n",
    "train, test = load_dataset(path)\n",
    "\n",
    "\n",
    "# function that build vocabulary with the token of all text\n",
    "def build_vocabulary(dataset, tokenizer, ngrams=1, min_freq=1):\n",
    "    # here we use counter to store the generated token to take in account the token frequency\n",
    "    counter = Counter()\n",
    "    # we iterate over all rows, covert text to word token, and add these token to bag_of words\n",
    "    for (label, line) in dataset:\n",
    "        counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line), ngrams=ngrams))\n",
    "    # sort the collected token counter by token's frequencies\n",
    "    sorted_by_token_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    # build a set of words as an orderedDict\n",
    "    words_dict = OrderedDict(sorted_by_token_freq_tuples)\n",
    "    # we build a vocabulary based on the words token\n",
    "    return vocab(words_dict, min_freq=min_freq)\n",
    "\n",
    "\n",
    "# build a vocab\n",
    "my_vocab = build_vocabulary(train, my_tokenizer)\n",
    "\n",
    "\n",
    "def encode(text, vocabulary, tokenizer):\n",
    "    return [vocabulary[word] for word in tokenizer(text)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.1 Padding the text tensor with zero\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This function read all tuples of the batch, and returns two tensors labels and features\n",
    "# The length of the text tensor is the max length of the text in the batch. For the text whose length\n",
    "# is inferior will be padded with 0.\n",
    "def padding_text(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label,\n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1], my_vocab, my_tokenizer) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len, v))\n",
    "    return (  # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] - 1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t), (0, l - len(t)), mode='constant', value=0) for t in v])\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=16, collate_fn=padding_text, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def select_hardware_for_training(device_name):\n",
    "    if device_name == 'cpu':\n",
    "        return 'cpu'\n",
    "    elif device_name == 'gpu':\n",
    "        return 'cuda' if (device_name == \"\") & torch.cuda.is_available() else 'cpu'\n",
    "    else:\n",
    "        print(\"Unknown device name, choose cpu as default device\")\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = select_hardware_for_training(\"cpu\")\n",
    "\n",
    "vocab_size = len(my_vocab)\n",
    "\n",
    "# build the model\n",
    "model_same_length = ClassifierWithSameLengthEmbedding(vocab_size, 32, len(label_classes)).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define training loop\n",
    "def train_loop(net, dataloader, lr=0.01, optimizer=None, loss_fn=torch.nn.CrossEntropyLoss(), epoch_size=None,\n",
    "               report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss, acc, count, i = 0, 0, 0, 0\n",
    "    for labels, features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out, labels)  #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        acc += (predicted == labels).sum()\n",
    "        count += len(labels)\n",
    "        i += 1\n",
    "        if i % report_freq == 0:\n",
    "            print(f\"{count}: acc={acc.item() / count}\")\n",
    "        if epoch_size and count > epoch_size:\n",
    "            break\n",
    "    return total_loss.item() / count, acc.item() / count\n",
    "\n",
    "\n",
    "# We are only training for 25k records here (less than one full epoch) for the sake of time, but you can continue\n",
    "# training, write a function to train for several epochs, and experiment with learning rate parameter to achieve\n",
    "# higher accuracy. You should be able to go to the accuracy of about 90%.\n",
    "train_loop(model_same_length, train_loader, lr=1, epoch_size=25000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.2 EmbeddingBag Layer and Variable-Length Sequence Representation\n",
    "\n",
    "In the previous architecture, we needed to pad all sequences to the same length in order to fit them into a\n",
    "mini-batch. This is not the most efficient way to represent variable length sequences - another approach would be\n",
    "to use offset vector, which would hold offsets of all sequences stored in one large vector.\n",
    "\n",
    "To work with offset representation, we use EmbeddingBag layer\n",
    "(https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html). It is similar to Embedding, but it takes\n",
    "content vector and offset vector as input, and it also includes averaging layer, which can be mean, sum or max."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ClassifierWithOffsetEmbedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define a collate_fn which returns tensors with offset\n",
    "def offset_text(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1], my_vocab, my_tokenizer)) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return (\n",
    "        torch.LongTensor([t[0] - 1 for t in b]),  # labels\n",
    "        torch.cat(x),  # text\n",
    "        o\n",
    "    )\n",
    "\n",
    "# get the data loader with the offset_text collate function\n",
    "train_loader_offset = torch.utils.data.DataLoader(train, batch_size=16, collate_fn=offset_text, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build model with the offset embedding\n",
    "model_with_offset = ClassifierWithOffsetEmbedding(vocab_size, 32, len(label_classes)).to(device)\n",
    "\n",
    "# define a train loop for offset\n",
    "def train_loop_offset(model, dataloader, lr=0.01, optimizer=None, loss_fn=torch.nn.CrossEntropyLoss(), epoch_size=None,\n",
    "                    report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    model.train()\n",
    "    total_loss, acc, count, i = 0, 0, 0, 0\n",
    "    for labels, text, off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels, text, off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = model(text, off)\n",
    "        loss = loss_fn(out, labels)  #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        acc += (predicted == labels).sum()\n",
    "        count += len(labels)\n",
    "        i += 1\n",
    "        if i % report_freq == 0:\n",
    "            print(f\"{count}: acc={acc.item() / count}\")\n",
    "        if epoch_size and count > epoch_size:\n",
    "            break\n",
    "    return total_loss.item() / count, acc.item() / count\n",
    "\n",
    "# train the model\n",
    "train_loader_offset(model_with_offset, train_loader_offset, lr=4, epoch_size=25000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Semantic Embedding Word2Vec\n",
    "In our previous example, the model embedding layer learnt to map words to vector representation,\n",
    "however, this representation did not have much semantically meaning. It would be nice to learn\n",
    "such vector representation, that similar words or synonyms would correspond to vectors that are\n",
    "close to each other in terms of some vector distance (eg. euclidean distance).\n",
    "\n",
    "To do that, we need to pre-train our embedding model on a large collection of text in a specific way.\n",
    "One of the first ways to train semantic embeddings is called Word2Vec. It is based on two main\n",
    "architectures that are used to produce a distributed representation of words:\n",
    "\n",
    "- Continuous bag-of-words (CBoW) — in this architecture, we train the model to predict a word from surrounding context.\n",
    "  Given the ngram (W−2,W−1,W0,W1,W2), the goal of the model is to predict W0 from (W−2,W−1,W1,W2).\n",
    "- Continuous skip-gram is opposite to CBoW. The model uses surrounding window of context words to predict the\n",
    "  current word.\n",
    "\n",
    "![word2vec](notebooks/img/example-algorithms-for-converting-words-to-vectors.png)\n",
    "\n",
    "CBoW is faster, while skip-gram is slower, but does a better job of representing infrequent words.\n",
    "\n",
    "Both CBOW and Skip-Grams are “predictive” embeddings, in that they only take local contexts into account. Word2Vec\n",
    "does not take advantage of global context.\n",
    "\n",
    "FastText, builds on Word2Vec by learning vector representations for each word and the charachter n-grams found\n",
    "within each word. The values of the representations are then averaged into one vector at each training step.\n",
    "While this adds a lot of additional computation to pre-training it enables word embeddings to encode sub-word\n",
    "information.\n",
    "\n",
    "Another method, GloVe, leverages the idea of co-occurrence matrix, uses neural methods to decompose co-occurrence\n",
    "matrix into more expressive and non linear word vectors.\n",
    "\n",
    "You can play with the example by changing embeddings to FastText and GloVe, since gensim supports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4.1\n",
    "\n",
    "To experiment with word2vec embedding pre-trained on Google News dataset, we can use gensim library.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_4857/4161500141.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# Below we find the words most similar to 'neural'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mw\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mw2v\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmost_similar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'neural'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{w} -> {p}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git/PyTorchTuto/venv/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36mmost_similar\u001B[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001B[0m\n\u001B[1;32m    733\u001B[0m             \u001B[0mnegative\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    734\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 735\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfill_norms\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    736\u001B[0m         \u001B[0mclip_end\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclip_end\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvectors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    737\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git/PyTorchTuto/venv/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36mfill_norms\u001B[0;34m(self, force)\u001B[0m\n\u001B[1;32m    617\u001B[0m         \"\"\"\n\u001B[1;32m    618\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorms\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mforce\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 619\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinalg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvectors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    620\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    621\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mnorm\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m~/git/PyTorchTuto/venv/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001B[0m in \u001B[0;36mnorm\u001B[0;34m(x, ord, axis, keepdims)\u001B[0m\n\u001B[1;32m   2558\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mord\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mord\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2559\u001B[0m             \u001B[0;31m# special case for speedup\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2560\u001B[0;31m             \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreal\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2561\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkeepdims\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkeepdims\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2562\u001B[0m         \u001B[0;31m# None of the str-type keywords for ord ('fro', 'nuc')\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mMemoryError\u001B[0m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "# load the google word2vec embedding, the size of the embedding is 1.6GB. So it may take some time to download\n",
    "w2v = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Below we find the words most similar to 'neural'\n",
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We can also extract vector embeddings from the word, to be used in training classification model\n",
    "# we only show first 20 components of the vector for clarity:\n",
    "\n",
    "w2v.word_vec('play')[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_4857/3322411890.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# and woman, and as far away from the word man\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mw2v\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmost_similar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpositive\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'king'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'woman'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnegative\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'man'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'w2v' is not defined"
     ]
    }
   ],
   "source": [
    "# Great thing about semantical embeddings is that you can manipulate vector encoding to change the semantics.\n",
    "# For example, we can ask to find a word, whose vector representation would be as close as possible to words king\n",
    "# and woman, and as far away from the word man\n",
    "\n",
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 Using Pre-Trained Embeddings in PyTorch\n",
    "\n",
    "We can modify the example above to pre-populate the matrix in our embedding layer with semantically embeddings,\n",
    "such as Word2Vec. We need to take into account that vocabularies of pre-trained embedding and our text corpus\n",
    "will likely not match, so we will initialize weights for the missing words with random values:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = ClassifierWithSameLengthEmbedding(vocab_size,embed_size,len(label_classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "\n",
    "for i,w in enumerate(vocab.itos):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}