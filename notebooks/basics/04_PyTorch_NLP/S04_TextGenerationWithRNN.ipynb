{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Generative networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short Term Memory Cells (LSTMs) and Gated\n",
    "Recurrent Units (GRUs) provided a mechanism for language modeling, i.e. they can learn word ordering and provide\n",
    "predictions for next word in a sequence. This allows us to use RNNs for generative tasks, such as ordinary text\n",
    "generation, machine translation, and even image captioning.\n",
    "\n",
    "In the RNN architecture of our previous section, each RNN unit produced the next hidden state as an output. However,\n",
    "we can also add another output to each recurrent unit, which would allow us to output a sequence (which is equal\n",
    "in length to the original sequence). Moreover, we can use RNN units that do not accept an input at each step, and\n",
    "just take some initial state vector, and then produce a sequence of outputs.\n",
    "\n",
    "This allows for different neural architectures that are shown in the picture below:\n",
    "![various_rnn](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/generative-rnn.jpg)\n",
    "Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red,\n",
    "output vectors are in blue and green vectors hold the RNN's state.\n",
    "\n",
    "- One-to-one is a traditional neural network with one input and one output\n",
    "- One-to-many is a generative architecture that accepts one input value, and generates a sequence of output values. For example, if we want to train image captioning network that would produce a textual description of a picture, we can a picture as input, pass it through CNN to obtain hidden state, and then have recurrent chain generate caption word-by-word\n",
    "- Many-to-one corresponds to RNN architectures we described in the previous unit, such as text classification\n",
    "- Many-to-many, or sequence-to-sequence corresponds to tasks such as machine translation, where we have first RNN collect all information from the input sequence into the hidden state, and another RNN chain unrolls this state into the output sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}