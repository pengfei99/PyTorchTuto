{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Generative networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short Term Memory Cells (LSTMs) and Gated\n",
    "Recurrent Units (GRUs) provided a mechanism for language modeling, i.e. they can learn word ordering and provide\n",
    "predictions for next word in a sequence. This allows us to use RNNs for generative tasks, such as ordinary text\n",
    "generation, machine translation, and even image captioning.\n",
    "\n",
    "In the RNN architecture of our previous section, each RNN unit produced the next hidden state as an output. However,\n",
    "we can also add another output to each recurrent unit, which would allow us to output a sequence (which is equal\n",
    "in length to the original sequence). Moreover, we can use RNN units that do not accept an input at each step, and\n",
    "just take some initial state vector, and then produce a sequence of outputs.\n",
    "\n",
    "This allows for different neural architectures that are shown in the picture below:\n",
    "![generative_rnn](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/generative-rnn.jpg)\n",
    "Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red,\n",
    "output vectors are in blue and green vectors hold the RNN's state (more on this soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}