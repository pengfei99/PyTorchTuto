{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Generative networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short Term Memory Cells (LSTMs) and Gated\n",
    "Recurrent Units (GRUs) provided a mechanism for language modeling, i.e. they can learn word ordering and provide\n",
    "predictions for next word in a sequence. This allows us to use RNNs for generative tasks, such as ordinary text\n",
    "generation, machine translation, and even image captioning.\n",
    "\n",
    "In the RNN architecture of our previous section, each RNN unit produced the next hidden state as an output. However,\n",
    "we can also add another output to each recurrent unit, which would allow us to output a sequence (which is equal\n",
    "in length to the original sequence). Moreover, we can use RNN units that do not accept an input at each step, and\n",
    "just take some initial state vector, and then produce a sequence of outputs.\n",
    "\n",
    "This allows for different neural architectures that are shown in the picture below:\n",
    "![various_rnn](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/various-rnn-architecture.jpg)\n",
    "Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red,\n",
    "output vectors are in blue and green vectors hold the RNN's state.\n",
    "\n",
    "- One-to-one is a traditional neural network with one input and one output\n",
    "- One-to-many is a generative architecture that accepts one input value, and generates a sequence of output values. For example, if we want to train image captioning network that would produce a textual description of a picture, we can a picture as input, pass it through CNN to obtain hidden state, and then have recurrent chain generate caption word-by-word\n",
    "- Many-to-one corresponds to RNN architectures we described in the previous unit, such as text classification\n",
    "- Many-to-many, or sequence-to-sequence corresponds to tasks such as machine translation, where we have first RNN collect all information from the input sequence into the hidden state, and another RNN chain unrolls this state into the output sequence.\n",
    "\n",
    "For more info on various rnn, http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "\n",
    "In this section, we will focus on simple generative models that help us to generate text. For simplicity, let's build\n",
    "character-level network, which generates text letter by letter. During training, we need to take some text corpus,\n",
    "and split it into letter sequences.\n",
    "\n",
    "We still use the news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "def load_dataset(storage_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root=storage_path)\n",
    "    train_dataset = list(train_dataset)\n",
    "    test_dataset = list(test_dataset)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "path = \"/tmp/pytorch/data\"\n",
    "train, test = load_dataset(path)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# build label class list\n",
    "label_classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Building character vocabulary\n",
    "\n",
    "To build character-level generative network, we need to split text into individual characters instead of words.\n",
    "This can be done by defining a different tokenizer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# build a simple tokenizer\n",
    "def char_tokenizer(words):\n",
    "    return list(words)  #[word for word in words]\n",
    "\n",
    "\n",
    "# function that build vocabulary with the token of all text\n",
    "def build_char_vocabulary(dataset, ngrams=1, min_freq=1):\n",
    "    # here we use counter to store the generated token to take in account the token frequency\n",
    "    counter = Counter()\n",
    "    # we iterate over all rows, covert text to word token, and add these token to bag_of words\n",
    "    for (label, line) in dataset:\n",
    "        counter.update(torchtext.data.utils.ngrams_iterator(char_tokenizer(line), ngrams=ngrams))\n",
    "    # sort the collected token counter by token's frequencies\n",
    "    sorted_by_token_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    # build a set of words as an orderedDict\n",
    "    words_dict = OrderedDict(sorted_by_token_freq_tuples)\n",
    "    # we build a vocabulary based on the words token\n",
    "    return vocab(words_dict, min_freq=min_freq)\n",
    "\n",
    "\n",
    "# build a character vocab\n",
    "my_vocab = build_char_vocabulary(train)\n",
    "my_vocab_size = len(my_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 82\n",
      "Encoding of 'a' is 2\n",
      "Character with code 13 is u\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size = {my_vocab_size}\")\n",
    "print(f\"Encoding of 'a' is {my_vocab.get_stoi()['a']}\")\n",
    "print(f\"Character with code 13 is {my_vocab.get_itos()[13]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Building an encoder\n",
    "This encoder can translate a text sequence to a tensor by using the character vocabulary that we built above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "generated tensor: tensor([41,  2,  9,  9,  0, 24,  3, 21,  0, 36,  1,  2,  8,  7,  0, 29,  9,  2,\n",
      "        19,  0, 36,  2, 12, 23,  0, 32,  6,  3,  4,  0,  3, 11,  1,  0, 36,  9,\n",
      "         2, 12, 23,  0, 53, 35,  1, 13,  3,  1,  8,  7, 54,  0, 35,  1, 13,  3,\n",
      "         1,  8,  7,  0, 27,  0, 24, 11,  4,  8,  3, 27,  7,  1,  9,  9,  1,  8,\n",
      "         7, 25,  0, 41,  2,  9,  9,  0, 24,  3,  8,  1,  1,  3, 56,  7,  0, 10,\n",
      "        19,  5,  6, 10,  9,  5,  6, 16, 59, 20,  2,  6, 10,  0,  4, 17,  0, 13,\n",
      "         9,  3,  8,  2, 27, 12, 18,  6,  5, 12,  7, 25,  0,  2,  8,  1,  0,  7,\n",
      "         1,  1,  5,  6, 16,  0, 16,  8,  1,  1,  6,  0,  2, 16,  2,  5,  6, 21])\n"
     ]
    }
   ],
   "source": [
    "# This encoder use a char vocabulary, and tokenizer instead of a word.\n",
    "def char_encode(text, char_vocab, tokenizer):\n",
    "    return [char_vocab[char] for char in tokenizer(text)]\n",
    "\n",
    "\n",
    "# convert text to tensor\n",
    "def text_to_tensor(x):\n",
    "    return torch.LongTensor(char_encode(x, my_vocab, tokenizer=char_tokenizer))\n",
    "\n",
    "\n",
    "# show an example\n",
    "print(f\"source text: {train[0][1]}\")\n",
    "print(f\"generated tensor: {text_to_tensor(train[0][1])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 Training a generative RNN\n",
    "\n",
    "The way we will train RNN to generate text is the following. On each step, we will take a sequence of characters of\n",
    "length n_chars, and ask the network to generate next output character for each input character\n",
    "\n",
    "![generate_rnn](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/rnn-generate.png)\n",
    "\n",
    "Depending on the actual scenario, we may also want to include some special characters, such as end-of-sequence <eos>.\n",
    "In our case, we just want to train the network for endless text generation, thus we will fix the size of each\n",
    "sequence to be equal to n_chars tokens. Consequently, each training example will consist of n_chars inputs and n_chars\n",
    "outputs (which are input sequence shifted one symbol to the left). Mini_batch will consist of several such sequences.\n",
    "\n",
    "The way we will generate mini_batches is to take each news text of length l, and generate all possible input-output\n",
    "combinations from it (there will be l-nchars such combinations). They will form one mini-batch, and size of\n",
    "mini-batches would be different at each training step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[41,  2,  9,  ..., 16, 59, 20],\n         [ 2,  9,  9,  ..., 59, 20,  2],\n         [ 9,  9,  0,  ..., 20,  2,  6],\n         ...,\n         [35,  1, 13,  ...,  2, 16,  2],\n         [ 1, 13,  3,  ..., 16,  2,  5],\n         [13,  3,  1,  ...,  2,  5,  6]]),\n tensor([[ 2,  9,  9,  ..., 59, 20,  2],\n         [ 9,  9,  0,  ..., 20,  2,  6],\n         [ 9,  0, 24,  ...,  2,  6, 10],\n         ...,\n         [ 1, 13,  3,  ..., 16,  2,  5],\n         [13,  3,  1,  ...,  2,  5,  6],\n         [ 3,  1,  8,  ...,  5,  6, 21]]))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chars = 100\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "def get_batch(s, nchars=n_chars):\n",
    "    ins = torch.zeros(len(s) - nchars, nchars, dtype=torch.long, device=device)\n",
    "    outs = torch.zeros(len(s) - nchars, nchars, dtype=torch.long, device=device)\n",
    "    for i in range(len(s) - nchars):\n",
    "        ins[i] = text_to_tensor(s[i:i + nchars])\n",
    "        outs[i] = text_to_tensor(s[i + 1:i + nchars + 1])\n",
    "    return ins, outs\n",
    "\n",
    "\n",
    "get_batch(train[0][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3.1 Build a generative model\n",
    "Now let's define the generator model. It can be based on any recurrent cell which we discussed in the previous section\n",
    "(simple rnn, LSTM or GRU). In this example we will use LSTM.\n",
    "\n",
    "Because the network takes characters as input, and vocabulary size is pretty small, we do not need embedding layer,\n",
    "one-hot-encoded input can directly go to LSTM cell. However, because we pass character numbers as input, we need to\n",
    "one-hot-encode them before passing to LSTM. This is done by calling one_hot function during forward pass. Output\n",
    "encoder would be a linear layer that will convert hidden state into one-hot-encoded output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class LSTMGenerator(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.rnn = torch.nn.LSTM(vocab_size, hidden_dim, batch_first=True)\n",
    "        # fc stands for fully connected layer\n",
    "        self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, s=None):\n",
    "        # we need to one-hot-encode character number\n",
    "        x = torch.nn.functional.one_hot(x, self.vocab_size).to(torch.float32)\n",
    "        x, s = self.rnn(x, s)\n",
    "        return self.fc(x), s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "During training, we want to be able to sample generated text. To do that, we will define generate function that will\n",
    "produce output string of length size, starting from the initial string start.\n",
    "\n",
    "The way it works is the following. First, we will pass the whole start string through the network, and take output\n",
    "state **s** and next predicted character out. Since out is one-hot encoded, we take argmax to get the index of the\n",
    "character **nc** in the vocabulary, and use **itos** to figure out the actual character and append it to the resulting list\n",
    "of characters chars. This process of generating one character is repeated size times to generate required number\n",
    "of characters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def generate(generative_model, vocabulary, size=100, start='today '):\n",
    "    chars = list(start)\n",
    "    output, state = generative_model(text_to_tensor(chars).view(1, -1).to(device))\n",
    "    for i in range(size):\n",
    "        nc = torch.argmax(output[0][-1])\n",
    "        chars.append(vocabulary.get_itos()[nc])\n",
    "        output, state = generative_model(nc.view(1, -1), state)\n",
    "    return ''.join(chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss = 4.416311740875244\n",
      "today daaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "Current loss = 2.1311776638031006\n",
      "today and a the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "Current loss = 1.6439659595489502\n",
      "today and a deling a deside a deal a deside a deal a deside a deal a deside a deal a deside a deal a desid\n",
      "Current loss = 2.417975425720215\n",
      "today and the United Stater and the United Stater and the United Stater and the United Stater and the Unit\n",
      "Current loss = 1.6155403852462769\n",
      "today and the start to the company to the company to the company to the company to the company to the comp\n",
      "Current loss = 1.7325990200042725\n",
      "today to a second the second the second the second the second the second the second the second the second \n",
      "Current loss = 1.905916690826416\n",
      "today and the final the first the first the first the first the first the first the first the first the fi\n",
      "Current loss = 1.8502205610275269\n",
      "today and and and and a stock and a stock and a stock and a stock and a stock and a stock and a stock and \n",
      "Current loss = 1.8434975147247314\n",
      "today and and and and and and and and and and and and and and and and and and and and and and and and and \n",
      "Current loss = 1.4979225397109985\n",
      "today and Tuesday and the service of the service of the service of the service of the service of the servi\n",
      "Current loss = 1.5564225912094116\n",
      "today to the straight to the straight and the straight and the straight and the straight and the straight \n"
     ]
    }
   ],
   "source": [
    "my_generative_model = LSTMGenerator(my_vocab_size, 64).to(device)\n",
    "\n",
    "samples_to_train = 10000\n",
    "optimizer = torch.optim.Adam(my_generative_model.parameters(), 0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "my_generative_model.train()\n",
    "for i, x in enumerate(train):\n",
    "    # x[0] is class label, x[1] is text\n",
    "    if len(x[1]) - n_chars < 10:\n",
    "        continue\n",
    "    samples_to_train -= 1\n",
    "    if not samples_to_train: break\n",
    "    text_in, text_out = get_batch(x[1])\n",
    "    optimizer.zero_grad()\n",
    "    out, s = my_generative_model(text_in)\n",
    "    loss = torch.nn.functional.cross_entropy(out.view(-1, my_vocab_size),\n",
    "                                             text_out.flatten())  #cross_entropy(out,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Current loss = {loss.item()}\")\n",
    "        print(generate(my_generative_model, my_vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above example already generates some pretty good text, but it can be further improved in several ways:\n",
    "\n",
    "- **Better mini_batch generation**: The way we prepared data for training was to generate one mini_batch from one\n",
    "sample. This is not ideal, because minibatches are all of different sizes, and some of them even cannot be generated,\n",
    "because the text is smaller than nchars. Also, small minibatches do not load GPU sufficiently enough. It would be\n",
    "wiser to get one large chunk of text from all samples, then generate all input-output pairs, shuffle them, and\n",
    "generate mini_batches of equal size.\n",
    "- **Multilayer LSTM**: It makes sense to try 2 or 3 layers of LSTM cells. As we mentioned in the previous section,\n",
    "each layer of LSTM extracts certain patterns from text, and in case of character-level generator we can expect\n",
    "lower LSTM level to be responsible for extracting syllables, and higher levels - for words and word combinations.\n",
    "This can be simply implemented by passing number-of-layers parameter to LSTM constructor.\n",
    "- **GRU units**: You may also want to experiment with GRU units and see which ones perform better, and with\n",
    "different hidden layer sizes. Too large hidden layer may result in over_fitting (e.g. network will learn exact text),\n",
    "and smaller size might not produce good result."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 Soft text generation and temperature\n",
    "\n",
    "In the previous example, when we generate charcater, we were always taking the character with highest probability as\n",
    "the next character in generated text. This resulted in the fact that the text often \"cycled\" between the same\n",
    "character sequences again and again, like in this example:\n",
    "\n",
    "\"today to a second the second the second the second the second the second the second the second\"\n",
    "\n",
    "However, if we look at the probability distribution for the next character, it could be that the difference between\n",
    "a few highest probabilities is not huge, e.g. one character can have probability 0.2, another - 0.19, etc. For example,\n",
    "when looking for the next character in the sequence 'play', next character can equally well be either 'space', or 'e'\n",
    "(as in the word player).\n",
    "\n",
    "This leads us to the conclusion that it is not always \"fair\" to select the character with higher probability, because\n",
    "choosing the second highest might still lead us to meaningful text. It is more wise to sample characters from the\n",
    "probability distribution given by the network output.\n",
    "\n",
    "This sampling can be done using multinomial function that implements so-called multinomial distribution. A function\n",
    "that implements this soft text generation is defined below:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Temperature = 0.3\n",
      "Today said to last for the starth and a the the first the serviration as the straight with the straight and the the sign the gand the controce the pay his company with the manages to and the computer to the manage and the win the straight of the straight to has the last the the relat the investors a with \n",
      "\n",
      "--- Temperature = 0.8\n",
      "Today to even profits a Dates the a thing the compat world found telephes, have and matchnoces fall and beats he the July the country in Google build-wiound a United Porting of victang the early #39;s schapi-provorter a there rise, id macritially stash as all has noman to cir new selmped to policos the th\n",
      "\n",
      "--- Temperature = 1.0\n",
      "Today lauly barting Olymsabive agreed on Webbly Fran Paigua Gurs, in 4 yesterday and same have team to cull beek patforering those a Frand back Web Kharta NR.Pje?tirsly wretain in fuchades one frocing that Bost sinal procectimt NEW YORENTOBB. throuch fout polis wireworth vace lay amolom in leats of a the \n",
      "\n",
      "--- Temperature = 1.3\n",
      "Today network into the 0.6 95 ismen, Peact sayer, manify Indug Imang seadying vey,\\Shoop Fairols Winnass Gryal, can Olympialoar found caply reef Aurliol Suss #39;s challeds that Fleaper waves cuh bratighe Coldaiculcis Surf Serchmony Timi-SHS - Bang Prilianery (WSBNL-27 U. 2HP9 (CAFMGr - fromp to eqous. Ma\n",
      "\n",
      "--- Temperature = 1.8\n",
      "Today W\\YOMS Britsets was, as Jds iPbowisdly ml-qorfedt-hourmwe-82-esribu' trenual thats 2 (Neh etheese Ne3doube heirver his suckecals-rolte-foomurXed raMusicitarahan cach ViligaltBC Howy17-$2\\von-Ethrers whathovae nickrays hhie7'roo one-mon-inuetisi STP Wernbrow I 40d cunes, horistn bworks: Nsivee 1Wrecn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_soft(generative_model, vocabulary, size=100,start='today ',temperature=1.0):\n",
    "    chars = list(start)\n",
    "    output, state = generative_model(text_to_tensor(chars).view(1,-1).to(device))\n",
    "    for i in range(size):\n",
    "        #nc = torch.argmax(out[0][-1])\n",
    "        out_dist = output[0][-1].div(temperature).exp()\n",
    "        # multinomial function that implements so-called multinomial distribution.\n",
    "        number_of_char = torch.multinomial(out_dist,1)[0]\n",
    "        chars.append(vocabulary.get_itos()[number_of_char])\n",
    "        output, state = generative_model(number_of_char.view(1,-1),state)\n",
    "    return ''.join(chars)\n",
    "\n",
    "for i in [0.3,0.8,1.0,1.3,1.8]:\n",
    "    print(f\"--- Temperature = {i}\\n{generate_soft(my_generative_model,my_vocab,size=300,start='Today ',temperature=i)}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have introduced one more parameter called temperature, which is used to indicate how hard we should stick to the\n",
    "highest probability. If temperature is 1.0, we do fair multinomial sampling, and when temperature goes to infinity -\n",
    "all probabilities become equal, and we randomly select next character. In the example below we can observe that the\n",
    "text becomes meaningless when we increase the temperature too much, and it resembles \"cycled\" hard-generated text\n",
    "when it becomes closer to 0."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}