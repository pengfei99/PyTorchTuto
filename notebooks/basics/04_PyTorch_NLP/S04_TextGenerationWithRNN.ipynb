{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Generative networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short Term Memory Cells (LSTMs) and Gated\n",
    "Recurrent Units (GRUs) provided a mechanism for language modeling, i.e. they can learn word ordering and provide\n",
    "predictions for next word in a sequence. This allows us to use RNNs for generative tasks, such as ordinary text\n",
    "generation, machine translation, and even image captioning.\n",
    "\n",
    "In the RNN architecture of our previous section, each RNN unit produced the next hidden state as an output. However,\n",
    "we can also add another output to each recurrent unit, which would allow us to output a sequence (which is equal\n",
    "in length to the original sequence). Moreover, we can use RNN units that do not accept an input at each step, and\n",
    "just take some initial state vector, and then produce a sequence of outputs.\n",
    "\n",
    "This allows for different neural architectures that are shown in the picture below:\n",
    "![various_rnn](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/various-rnn-architecture.jpg)\n",
    "Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red,\n",
    "output vectors are in blue and green vectors hold the RNN's state.\n",
    "\n",
    "- One-to-one is a traditional neural network with one input and one output\n",
    "- One-to-many is a generative architecture that accepts one input value, and generates a sequence of output values. For example, if we want to train image captioning network that would produce a textual description of a picture, we can a picture as input, pass it through CNN to obtain hidden state, and then have recurrent chain generate caption word-by-word\n",
    "- Many-to-one corresponds to RNN architectures we described in the previous unit, such as text classification\n",
    "- Many-to-many, or sequence-to-sequence corresponds to tasks such as machine translation, where we have first RNN collect all information from the input sequence into the hidden state, and another RNN chain unrolls this state into the output sequence.\n",
    "\n",
    "For more info on various rnn, http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "\n",
    "In this section, we will focus on simple generative models that help us to generate text. For simplicity, let's build\n",
    "character-level network, which generates text letter by letter. During training, we need to take some text corpus,\n",
    "and split it into letter sequences.\n",
    "\n",
    "We still use the news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "def load_dataset(storage_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root=storage_path)\n",
    "    train_dataset = list(train_dataset)\n",
    "    test_dataset = list(test_dataset)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "path = \"/tmp/pytorch/data\"\n",
    "train, test = load_dataset(path)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# build label class list\n",
    "label_classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Building character vocabulary\n",
    "\n",
    "To build character-level generative network, we need to split text into individual characters instead of words.\n",
    "This can be done by defining a different tokenizer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# build a simple tokenizer\n",
    "def char_tokenizer(words):\n",
    "    return list(words) #[word for word in words]\n",
    "\n",
    "\n",
    "# function that build vocabulary with the token of all text\n",
    "def build_char_vocabulary(dataset, ngrams=1, min_freq=1):\n",
    "    # here we use counter to store the generated token to take in account the token frequency\n",
    "    counter = Counter()\n",
    "    # we iterate over all rows, covert text to word token, and add these token to bag_of words\n",
    "    for (label, line) in dataset:\n",
    "        counter.update(torchtext.data.utils.ngrams_iterator(char_tokenizer(line), ngrams=ngrams))\n",
    "    # sort the collected token counter by token's frequencies\n",
    "    sorted_by_token_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    # build a set of words as an orderedDict\n",
    "    words_dict = OrderedDict(sorted_by_token_freq_tuples)\n",
    "    # we build a vocabulary based on the words token\n",
    "    return vocab(words_dict, min_freq=min_freq)\n",
    "\n",
    "# build a character vocab\n",
    "my_vocab = build_char_vocabulary(train)\n",
    "my_vocab_size = len(my_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 82\n",
      "Encoding of 'a' is 2\n",
      "Character with code 13 is u\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size = {my_vocab_size}\")\n",
    "print(f\"Encoding of 'a' is {my_vocab.get_stoi()['a']}\")\n",
    "print(f\"Character with code 13 is {my_vocab.get_itos()[13]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Building an encoder\n",
    "This encoder can translate a text sequence to a tensor by using the character vocabulary that we built above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "generated tensor: tensor([41,  2,  9,  9,  0, 24,  3, 21,  0, 36,  1,  2,  8,  7,  0, 29,  9,  2,\n",
      "        19,  0, 36,  2, 12, 23,  0, 32,  6,  3,  4,  0,  3, 11,  1,  0, 36,  9,\n",
      "         2, 12, 23,  0, 53, 35,  1, 13,  3,  1,  8,  7, 54,  0, 35,  1, 13,  3,\n",
      "         1,  8,  7,  0, 27,  0, 24, 11,  4,  8,  3, 27,  7,  1,  9,  9,  1,  8,\n",
      "         7, 25,  0, 41,  2,  9,  9,  0, 24,  3,  8,  1,  1,  3, 56,  7,  0, 10,\n",
      "        19,  5,  6, 10,  9,  5,  6, 16, 59, 20,  2,  6, 10,  0,  4, 17,  0, 13,\n",
      "         9,  3,  8,  2, 27, 12, 18,  6,  5, 12,  7, 25,  0,  2,  8,  1,  0,  7,\n",
      "         1,  1,  5,  6, 16,  0, 16,  8,  1,  1,  6,  0,  2, 16,  2,  5,  6, 21])\n"
     ]
    }
   ],
   "source": [
    "# This encoder use a char vocabulary, and tokenizer instead of a word.\n",
    "def char_encode(text, char_vocab, tokenizer):\n",
    "    return [char_vocab[char] for char in tokenizer(text)]\n",
    "\n",
    "# convert text to tensor\n",
    "def text_to_tensor(x):\n",
    "    return torch.LongTensor(char_encode(x,my_vocab,tokenizer=char_tokenizer))\n",
    "\n",
    "# show an example\n",
    "print(f\"source text: {train[0][1]}\")\n",
    "print(f\"generated tensor: {text_to_tensor(train[0][1])}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}