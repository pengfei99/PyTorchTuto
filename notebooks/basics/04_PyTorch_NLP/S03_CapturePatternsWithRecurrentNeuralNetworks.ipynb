{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Capture patterns with recurrent neural networks\n",
    "\n",
    "## 3.1 Recurrent neural networks\n",
    "In the previous module, we have been using rich semantic representations of text, and a simple linear classifier on\n",
    "top of the embeddings. What this architecture does is to capture aggregated meaning of words in a sentence, but it\n",
    "does not take into account the order of words, because aggregation operation on top of embeddings removed this\n",
    "information from the original text. Because these models are unable to model word ordering, they cannot solve\n",
    "more complex or ambiguous tasks such as text generation or question answering.\n",
    "\n",
    "To capture the meaning of text sequence, we need to use another neural network architecture, which is called a\n",
    "**recurrent neural network, or RNN**. In RNN, we pass our sentence through the network one symbol at a time,\n",
    "and the network produces some state, which we then pass to the network again with the next symbol.\n",
    "\n",
    "![rnn-model](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img//sample-rnn-model-generation.png)\n",
    "\n",
    "Given the input sequence of tokens X0,...,Xn.  RNN creates a sequence of neural network blocks, and trains this\n",
    "sequence end-to-end using back propagation. Each network block takes a pair (Xi,Si)as an input, and produces S(i+1)\n",
    "as a result. Final state Sn or output Xn goes into a linear classifier to produce the result. All network blocks\n",
    "share the same weights, and are trained end-to-end using one backpropagation pass.\n",
    "\n",
    "Because state vectors S0,...,Sn are passed through the network, it is able to learn the sequential dependencies\n",
    "between words. For example, when the word not appears somewhere in the sequence, it can learn to negate certain\n",
    "elements within the state vector, resulting in negation."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 A simple example of RNN\n",
    "\n",
    "In this example, we will use a simple RNN to classifier the news data set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "def load_dataset(storage_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root=storage_path)\n",
    "    train_dataset = list(train_dataset)\n",
    "    test_dataset = list(test_dataset)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "path = \"/tmp/pytorch/data\"\n",
    "train, test = load_dataset(path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# build a simple tokenizer\n",
    "my_tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# build label class list\n",
    "label_classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "# function that build vocabulary with the token of all text\n",
    "def build_vocabulary(dataset, tokenizer, ngrams=1, min_freq=1):\n",
    "    # here we use counter to store the generated token to take in account the token frequency\n",
    "    counter = Counter()\n",
    "    # we iterate over all rows, covert text to word token, and add these token to bag_of words\n",
    "    for (label, line) in dataset:\n",
    "        counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line), ngrams=ngrams))\n",
    "    # sort the collected token counter by token's frequencies\n",
    "    sorted_by_token_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    # build a set of words as an orderedDict\n",
    "    words_dict = OrderedDict(sorted_by_token_freq_tuples)\n",
    "    # we build a vocabulary based on the words token\n",
    "    return vocab(words_dict, min_freq=min_freq)\n",
    "\n",
    "# build a vocab\n",
    "my_vocab = build_vocabulary(train, my_tokenizer)\n",
    "\n",
    "\n",
    "def encode(text, vocabulary, tokenizer):\n",
    "    return [vocabulary[word] for word in tokenizer(text)]\n",
    "\n",
    "my_vocab_size = len(my_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2.1 Build a Simple RNN Classifier\n",
    "\n",
    "In case of simple RNN, each recurrent unit is a simple linear network, which takes\n",
    "**concatenated input vector and state vector**, and produce a **new state vector**. PyTorch represents this unit\n",
    "with RNNCell class, and a networks of such cells - as RNN layer.\n",
    "\n",
    "To define an RNN classifier, we will first apply an embedding layer to lower the dimensionality of input vocabulary,\n",
    "and then have RNN layer on top of it.\n",
    "\n",
    "RNNs are quite difficult to train. So keep in mind two important points:\n",
    "1. **Small learning rate**: because once the RNN cells are unrolled along the sequence length, the\n",
    "resulting number of layers involved in back propagation is quite large.\n",
    "2. **Use GPU if you can**: It can take quite a long time to train the network on larger dataset to produce good results.\n",
    "                     GPU may reduce that training time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# In this classifier, we will use padded data loader, so each batch will have a number of padded sequences of the\n",
    "# same length. RNN layer will take the sequence of embedding tensors, and produce two outputs:\n",
    "# - x: is a sequence of RNN cell outputs at each step\n",
    "# - h: is a final hidden state for the last element of the sequence\n",
    "# We then apply a fully-connected linear classifier to get the number of class.\n",
    "class SimpleRNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # note our embedding layer is untrained, if you want better results we can use pre-trained embedding layer\n",
    "        # with Word2Vec or GloVe embeddings, as described in the previous unit. For better understanding, you might\n",
    "        # want to adapt this code to work with pre-trained embeddings.\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = torch.nn.RNN(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x,h = self.rnn(x)\n",
    "        return self.fc(x.mean(dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.2 Build a training loop\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def select_hardware_for_training(device_name):\n",
    "    if device_name == 'cpu':\n",
    "        return 'cpu'\n",
    "    elif device_name == 'gpu':\n",
    "        return 'cuda' if (device_name == \"\") & torch.cuda.is_available() else 'cpu'\n",
    "    else:\n",
    "        print(\"Unknown device name, choose cpu as default device\")\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = select_hardware_for_training(\"cpu\")\n",
    "\n",
    "# define training loop\n",
    "def train_loop(net, dataloader, lr=0.01, optimizer=None, loss_fn=torch.nn.CrossEntropyLoss(), epoch_size=None,\n",
    "               report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss, acc, count, i = 0, 0, 0, 0\n",
    "    for labels, features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out, labels)  #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        acc += (predicted == labels).sum()\n",
    "        count += len(labels)\n",
    "        i += 1\n",
    "        if i % report_freq == 0:\n",
    "            print(f\"{count}: acc={acc.item() / count}\")\n",
    "        if epoch_size and count > epoch_size:\n",
    "            break\n",
    "    return total_loss.item() / count, acc.item() / count\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def padding_text(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label,\n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1], my_vocab, my_tokenizer) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len, v))\n",
    "    return (  # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] - 1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t), (0, l - len(t)), mode='constant', value=0) for t in v])\n",
    "    )\n",
    "# build a dataloader with text padding tensor\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=16, collate_fn=padding_text, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.3134375\n",
      "6400: acc=0.38953125\n",
      "9600: acc=0.4515625\n",
      "12800: acc=0.505859375\n",
      "16000: acc=0.5450625\n",
      "19200: acc=0.5766666666666667\n",
      "22400: acc=0.6025446428571428\n",
      "25600: acc=0.626875\n",
      "28800: acc=0.6464583333333334\n",
      "32000: acc=0.6628125\n",
      "35200: acc=0.6766477272727273\n",
      "38400: acc=0.6892447916666666\n",
      "41600: acc=0.7001682692307692\n",
      "44800: acc=0.7108705357142857\n",
      "48000: acc=0.7194166666666667\n",
      "51200: acc=0.7271484375\n",
      "54400: acc=0.7348161764705883\n",
      "57600: acc=0.7415104166666666\n",
      "60800: acc=0.7479934210526316\n",
      "64000: acc=0.753375\n",
      "67200: acc=0.7586011904761905\n",
      "70400: acc=0.7637073863636363\n",
      "73600: acc=0.7682744565217391\n",
      "76800: acc=0.77265625\n",
      "80000: acc=0.776825\n",
      "83200: acc=0.7809735576923077\n",
      "86400: acc=0.7843518518518519\n",
      "89600: acc=0.787578125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3993/78839084.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mnet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSimpleRNNClassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmy_vocab_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m64\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabel_classes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrain_loop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.001\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_3993/986557374.py\u001B[0m in \u001B[0;36mtrain_loop\u001B[0;34m(net, dataloader, lr, optimizer, loss_fn, epoch_size, report_freq)\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m#cross_entropy(out,labels)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m         \u001B[0mtotal_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpredicted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git/PyTorchTuto/venv/lib/python3.8/site-packages/torch/optim/optimizer.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m                 \u001B[0mprofile_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Optimizer.step#{}.step\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecord_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprofile_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m                     \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git/PyTorchTuto/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git/PyTorchTuto/venv/lib/python3.8/site-packages/torch/optim/adam.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    105\u001B[0m                     \u001B[0mstate_steps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'step'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 107\u001B[0;31m             F.adam(params_with_grad,\n\u001B[0m\u001B[1;32m    108\u001B[0m                    \u001B[0mgrads\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m                    \u001B[0mexp_avgs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git/PyTorchTuto/venv/lib/python3.8/site-packages/torch/optim/_functional.py\u001B[0m in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001B[0m\n\u001B[1;32m     96\u001B[0m         \u001B[0mstep_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlr\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mbias_correction1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 98\u001B[0;31m         \u001B[0mparam\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maddcdiv_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexp_avg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdenom\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mstep_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "net = SimpleRNNClassifier(my_vocab_size,64,32,len(label_classes)).to(device)\n",
    "train_loop(net,train_loader, lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Long Short Term Memory (LSTM) in RNN\n",
    "\n",
    "One of the main problems of classical RNNs is so-called **vanishing gradients** problem. Because RNNs are\n",
    "trained end-to-end in one back-propagation pass, it is having hard times to back propagate error to the first\n",
    "layers of the network, and thus the network cannot learn relationships between distant tokens. One of the ways to\n",
    "avoid this problem is to introduce explicit state management by using so called **gates**. There are two most known\n",
    "architectures of this kind:\n",
    " - Long Short Term Memory (LSTM)\n",
    " - Gated Relay Unit (GRU)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}