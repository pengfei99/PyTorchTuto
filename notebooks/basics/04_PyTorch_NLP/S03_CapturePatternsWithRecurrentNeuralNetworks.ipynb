{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Capture patterns with recurrent neural networks\n",
    "\n",
    "## 3.1 Recurrent neural networks\n",
    "In the previous module, we have been using rich semantic representations of text, and a simple linear classifier on\n",
    "top of the embeddings. What this architecture does is to capture aggregated meaning of words in a sentence, but it\n",
    "does not take into account the order of words, because aggregation operation on top of embeddings removed this\n",
    "information from the original text. Because these models are unable to model word ordering, they cannot solve\n",
    "more complex or ambiguous tasks such as text generation or question answering.\n",
    "\n",
    "To capture the meaning of text sequence, we need to use another neural network architecture, which is called a\n",
    "**recurrent neural network, or RNN**. In RNN, we pass our sentence through the network one symbol at a time,\n",
    "and the network produces some state, which we then pass to the network again with the next symbol.\n",
    "\n",
    "![rnn-model](notebooks/img/sample-rnn-model-generation.png)\n",
    "\n",
    "Given the input sequence of tokens X0,...,Xn.  RNN creates a sequence of neural network blocks, and trains this \n",
    "sequence end-to-end using back propagation. Each network block takes a pair (Xi,Si)as an input, and produces S(i+1) \n",
    "as a result. Final state Sn or output Xn goes into a linear classifier to produce the result. All network blocks \n",
    "share the same weights, and are trained end-to-end using one backpropagation pass.\n",
    "\n",
    "Because state vectors S0,...,Sn are passed through the network, it is able to learn the sequential dependencies \n",
    "between words. For example, when the word not appears somewhere in the sequence, it can learn to negate certain \n",
    "elements within the state vector, resulting in negation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download data\n",
    "def load_dataset(storage_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root=storage_path)\n",
    "    train_dataset = list(train_dataset)\n",
    "    test_dataset = list(test_dataset)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "path = \"/tmp/pytorch/data\"\n",
    "train, test = load_dataset(path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build a simple tokenizer\n",
    "my_tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# build label class list\n",
    "label_classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "# function that build vocabulary with the token of all text\n",
    "def build_vocabulary(dataset, tokenizer, ngrams=1, min_freq=1):\n",
    "    # here we use counter to store the generated token to take in account the token frequency\n",
    "    counter = Counter()\n",
    "    # we iterate over all rows, covert text to word token, and add these token to bag_of words\n",
    "    for (label, line) in dataset:\n",
    "        counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line), ngrams=ngrams))\n",
    "    # sort the collected token counter by token's frequencies\n",
    "    sorted_by_token_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    # build a set of words as an orderedDict\n",
    "    words_dict = OrderedDict(sorted_by_token_freq_tuples)\n",
    "    # we build a vocabulary based on the words token\n",
    "    return vocab(words_dict, min_freq=min_freq)\n",
    "\n",
    "# build a vocab\n",
    "my_vocab = build_vocabulary(train, my_tokenizer)\n",
    "\n",
    "\n",
    "def encode(text, vocabulary, tokenizer):\n",
    "    return [vocabulary[word] for word in tokenizer(text)]\n",
    "\n",
    "vocab_size = len(my_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}