{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Capture patterns with recurrent neural networks\n",
    "\n",
    "## 3.1 Recurrent neural networks\n",
    "In the previous module, we have been using rich semantic representations of text, and a simple linear classifier on\n",
    "top of the embeddings. What this architecture does is to capture aggregated meaning of words in a sentence, but it\n",
    "does not take into account the order of words, because aggregation operation on top of embeddings removed this\n",
    "information from the original text. Because these models are unable to model word ordering, they cannot solve\n",
    "more complex or ambiguous tasks such as text generation or question answering.\n",
    "\n",
    "To capture the meaning of text sequence, we need to use another neural network architecture, which is called a\n",
    "**recurrent neural network, or RNN**. In RNN, we pass our sentence through the network one symbol at a time,\n",
    "and the network produces some state, which we then pass to the network again with the next symbol.\n",
    "\n",
    "![rnn-model](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/sample-rnn-model-generation.png)\n",
    "\n",
    "Given the input sequence of tokens X0,...,Xn.  RNN creates a sequence of neural network blocks, and trains this\n",
    "sequence end-to-end using back propagation. Each network block takes a pair (Xi,Si)as an input, and produces S(i+1)\n",
    "as a result. Final state Sn or output Xn goes into a linear classifier to produce the result. All network blocks\n",
    "share the same weights, and are trained end-to-end using one backpropagation pass.\n",
    "\n",
    "Because state vectors S0,...,Sn are passed through the network, it is able to learn the sequential dependencies\n",
    "between words. For example, when the word not appears somewhere in the sequence, it can learn to negate certain\n",
    "elements within the state vector, resulting in negation."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 A simple example of RNN\n",
    "\n",
    "In this example, we will use a simple RNN to classifier the news data set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "def load_dataset(storage_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root=storage_path)\n",
    "    train_dataset = list(train_dataset)\n",
    "    test_dataset = list(test_dataset)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "path = \"/tmp/pytorch/data\"\n",
    "train, test = load_dataset(path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# build a simple tokenizer\n",
    "my_tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# build label class list\n",
    "label_classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "\n",
    "# function that build vocabulary with the token of all text\n",
    "def build_vocabulary(dataset, tokenizer, ngrams=1, min_freq=1):\n",
    "    # here we use counter to store the generated token to take in account the token frequency\n",
    "    counter = Counter()\n",
    "    # we iterate over all rows, covert text to word token, and add these token to bag_of words\n",
    "    for (label, line) in dataset:\n",
    "        counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line), ngrams=ngrams))\n",
    "    # sort the collected token counter by token's frequencies\n",
    "    sorted_by_token_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    # build a set of words as an orderedDict\n",
    "    words_dict = OrderedDict(sorted_by_token_freq_tuples)\n",
    "    # we build a vocabulary based on the words token\n",
    "    return vocab(words_dict, min_freq=min_freq)\n",
    "\n",
    "\n",
    "# build a vocab\n",
    "my_vocab = build_vocabulary(train, my_tokenizer)\n",
    "\n",
    "\n",
    "def encode(text, vocabulary, tokenizer):\n",
    "    return [vocabulary[word] for word in tokenizer(text)]\n",
    "\n",
    "\n",
    "my_vocab_size = len(my_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2.1 Build a Simple RNN Classifier\n",
    "\n",
    "In case of simple RNN, each recurrent unit is a simple linear network, which takes\n",
    "**concatenated input vector and state vector**, and produce a **new state vector**. PyTorch represents this unit\n",
    "with RNNCell class, and a networks of such cells - as RNN layer.\n",
    "\n",
    "To define an RNN classifier, we will first apply an embedding layer to lower the dimensionality of input vocabulary,\n",
    "and then have RNN layer on top of it.\n",
    "\n",
    "RNNs are quite difficult to train. So keep in mind two important points:\n",
    "1. **Small learning rate**: because once the RNN cells are unrolled along the sequence length, the\n",
    "resulting number of layers involved in back propagation is quite large.\n",
    "2. **Use GPU if you can**: It can take quite a long time to train the network on larger dataset to produce good results.\n",
    "                     GPU may reduce that training time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# In this classifier, we will use padded data loader, so each batch will have a number of padded sequences of the\n",
    "# same length. RNN layer will take the sequence of embedding tensors, and produce two outputs:\n",
    "# - x: is a sequence of RNN cell outputs at each step\n",
    "# - h: is a final hidden state for the last element of the sequence\n",
    "# We then apply a fully-connected linear classifier to get the number of class.\n",
    "class SimpleRNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # note our embedding layer is untrained, if you want better results we can use pre-trained embedding layer\n",
    "        # with Word2Vec or GloVe embeddings, as described in the previous unit. For better understanding, you might\n",
    "        # want to adapt this code to work with pre-trained embeddings.\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = torch.nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x, h = self.rnn(x)\n",
    "        return self.fc(x.mean(dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.2 Build a training loop\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def select_hardware_for_training(device_name):\n",
    "    if device_name == 'cpu':\n",
    "        return 'cpu'\n",
    "    elif device_name == 'gpu':\n",
    "        return 'cuda' if (device_name == \"\") & torch.cuda.is_available() else 'cpu'\n",
    "    else:\n",
    "        print(\"Unknown device name, choose cpu as default device\")\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = select_hardware_for_training(\"cpu\")\n",
    "\n",
    "\n",
    "# define training loop\n",
    "def train_loop(net, dataloader, lr=0.01, optimizer=None, loss_fn=torch.nn.CrossEntropyLoss(), epoch_size=None,\n",
    "               report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss, acc, count, i = 0, 0, 0, 0\n",
    "    for labels, features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out, labels)  #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        acc += (predicted == labels).sum()\n",
    "        count += len(labels)\n",
    "        i += 1\n",
    "        if i % report_freq == 0:\n",
    "            print(f\"{count}: acc={acc.item() / count}\")\n",
    "        if epoch_size and count > epoch_size:\n",
    "            break\n",
    "    return total_loss.item() / count, acc.item() / count\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def padding_text(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label,\n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1], my_vocab, my_tokenizer) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len, v))\n",
    "    return (  # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] - 1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t), (0, l - len(t)), mode='constant', value=0) for t in v])\n",
    "    )\n",
    "\n",
    "\n",
    "# build a dataloader with text padding tensor\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=16, collate_fn=padding_text, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.32\n",
      "6400: acc=0.3978125\n",
      "9600: acc=0.47208333333333335\n",
      "12800: acc=0.525546875\n",
      "16000: acc=0.5631875\n",
      "19200: acc=0.5941666666666666\n",
      "22400: acc=0.6209375\n",
      "25600: acc=0.64015625\n",
      "28800: acc=0.6567013888888888\n",
      "32000: acc=0.672\n",
      "35200: acc=0.6851420454545455\n",
      "38400: acc=0.6969270833333333\n",
      "41600: acc=0.7058413461538462\n",
      "44800: acc=0.7149107142857143\n",
      "48000: acc=0.72375\n",
      "51200: acc=0.73171875\n",
      "54400: acc=0.73875\n",
      "57600: acc=0.7451215277777777\n",
      "60800: acc=0.7511348684210526\n",
      "64000: acc=0.75590625\n",
      "67200: acc=0.7608630952380953\n",
      "70400: acc=0.7652982954545454\n",
      "73600: acc=0.7694565217391305\n",
      "76800: acc=0.7738151041666667\n",
      "80000: acc=0.7781125\n",
      "83200: acc=0.7820913461538461\n",
      "86400: acc=0.7852430555555555\n",
      "89600: acc=0.7884821428571429\n",
      "92800: acc=0.7910883620689655\n",
      "96000: acc=0.7937395833333334\n",
      "99200: acc=0.7963508064516129\n",
      "102400: acc=0.798994140625\n",
      "105600: acc=0.8017992424242424\n",
      "108800: acc=0.8042738970588236\n",
      "112000: acc=0.8062589285714286\n",
      "115200: acc=0.8084809027777777\n",
      "118400: acc=0.8107094594594595\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.03263011678059896, 0.8118916666666667)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rnn_model = SimpleRNNClassifier(my_vocab_size, 64, 32, len(label_classes)).to(device)\n",
    "train_loop(simple_rnn_model, train_loader, lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Long Short Term Memory (LSTM) classifier\n",
    "\n",
    "One of the main problems of classical RNNs is so-called **vanishing gradients** problem. Because RNNs are\n",
    "trained end-to-end in one back-propagation pass, it is having hard times to back propagate error to the first\n",
    "layers of the network, and thus the network cannot learn relationships between distant tokens. One of the ways to\n",
    "avoid this problem is to introduce explicit state management by using so called **gates**. There are two most known\n",
    "architectures of this kind:\n",
    " - Long Short Term Memory (LSTM)\n",
    " - Gated Relay Unit (GRU)\n",
    "\n",
    "\n",
    "![LSTM-model](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/long-short-term-memory-cell.svg)\n",
    "\n",
    "LSTM Network is organized in a manner similar to RNN, but there are two states that are being passed from layer to\n",
    "layer:\n",
    "- **actual state**: c\n",
    "- **hidden vector**: h\n",
    "\n",
    "At each unit, hidden vector h(i) is concatenated with input x(i), and they control what happens to the state\n",
    "c via gates. Each gate is a neural network with sigmoid activation (output in the range [0,1]), which can be\n",
    "thought of as bitwise mask when multiplied by the state vector. There are the following gates (from left to\n",
    "right on the figure above):\n",
    "\n",
    "- forget gate:  takes hidden vector and determines, which components of the vector **c** we need to forget, and which to pass through.\n",
    "- input gate: takes some information from the input and hidden vector, and inserts it into state.\n",
    "- output gate: transforms state via some linear layer with tanh activation, then selects some of its components using hidden vector h(i) to produce new state c(i+1).\n",
    "\n",
    "Components of the state **c** can be thought of as some flags that can be switched on and off. For example, when we\n",
    "encounter a name Alice in the sequence, we may want to assume that it refers to female character, and raise the flag\n",
    "in the state that we have female noun in the sentence. When we further encounter phrases and Tom, we will raise the\n",
    "flag that we have plural noun. Thus by manipulating state we can supposedly keep track of grammatical properties of\n",
    "sentence parts.\n",
    "\n",
    "A great resource for understanding internals of LSTM is this great article by Christopher Olah,\n",
    "Understanding LSTM Networks: https://colah.github.io/posts/2015-08-Understanding-LSTMs/ .\n",
    "\n",
    "### 3.3.1 Build a LSTM classifier in PyTorch\n",
    "PyTorch hides the complexe implementation of LSTM inside LSTMCell class, and provides LSTM object to represent the\n",
    "whole LSTM layer. Thus, implementation of LSTM classifier will be pretty similar to the simple RNN which we have seen\n",
    "before."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data) - 0.5\n",
    "        self.rnn = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x, (h, c) = self.rnn(x)\n",
    "        return self.fc(h[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lstm_model = LSTMClassifier(my_vocab_size, 64, 32, len(label_classes)).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2 Train the LSTM classifier\n",
    "\n",
    "Note that training LSTM is also quite slow, and you may not seem much raise in accuracy in the beginning of training.\n",
    "Also, you may need to play with lr learning rate parameter to find the learning rate that results in reasonable\n",
    "training speed, and yet does not cause"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_loop(lstm_model, train_loader, lr=0.001)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.2540625\n",
      "6400: acc=0.2465625\n",
      "9600: acc=0.24729166666666666\n",
      "12800: acc=0.25046875\n",
      "16000: acc=0.2701875\n",
      "19200: acc=0.30786458333333333\n",
      "22400: acc=0.34776785714285713\n",
      "25600: acc=0.3819921875\n",
      "28800: acc=0.4117013888888889\n",
      "32000: acc=0.43503125\n",
      "35200: acc=0.4556818181818182\n",
      "38400: acc=0.47265625\n",
      "41600: acc=0.48677884615384615\n",
      "44800: acc=0.5008482142857142\n",
      "48000: acc=0.512875\n",
      "51200: acc=0.524921875\n",
      "54400: acc=0.5358823529411765\n",
      "57600: acc=0.5452430555555555\n",
      "60800: acc=0.5553289473684211\n",
      "64000: acc=0.566234375\n",
      "67200: acc=0.5773660714285714\n",
      "70400: acc=0.5890767045454546\n",
      "73600: acc=0.5998913043478261\n",
      "76800: acc=0.6101432291666666\n",
      "80000: acc=0.62015\n",
      "83200: acc=0.6291826923076923\n",
      "86400: acc=0.6374884259259259\n",
      "89600: acc=0.6453683035714286\n",
      "92800: acc=0.6530172413793104\n",
      "96000: acc=0.66021875\n",
      "99200: acc=0.6668044354838709\n",
      "102400: acc=0.67337890625\n",
      "105600: acc=0.6798484848484848\n",
      "108800: acc=0.6854319852941176\n",
      "112000: acc=0.6908482142857143\n",
      "115200: acc=0.6961024305555555\n",
      "118400: acc=0.701402027027027\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.041063944498697914, 0.7039083333333334)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Improve training loop by using packed sequences\n",
    "\n",
    "In previous training loop, we used dataloader that pad all sequences in the mini-batch with zero vectors. While it\n",
    "results in some memory waste, with RNNs it is more critical that additional RNN cells are created for the padded\n",
    "input items, which take part in training, yet do not carry any important input information. It would be much better\n",
    "to train RNN only to the actual sequence size.\n",
    "\n",
    "To do that, a special format of padded sequence storage is introduced in PyTorch. Suppose we have input padded\n",
    "mini-batch which looks like this:\n",
    "\n",
    "[[1,2,3,4,5],\n",
    " [6,7,8,0,0],\n",
    " [9,0,0,0,0]]\n",
    "\n",
    "Here 0 represents padded values, and the actual length vector of input sequences is [5,3,1].\n",
    "\n",
    "In order to effectively train RNN with padded sequence, we want to begin training first group of RNN cells with\n",
    "large mini-batch ([1,6,9]), but then end processing of third sequence, and continue training with shorted mini-batches\n",
    "([2,7], [3,8]), and so on. Thus, packed sequence is represented as one vector - in our case [1,6,9,2,7,3,8,4,5], and\n",
    "length vector ([5,3,1]), from which we can easily reconstruct the original padded mini-batch.\n",
    "\n",
    "To use packed sequence, we can use two function:\n",
    "- **torch.nn.utils.rnn.pack_padded_sequence**: encode a padded tensor to packed sequence tensor.\n",
    "- **torch.nn.utils.rnn.pad_packed_sequence**: decode a packed sequence tensor to a padded tensor\n",
    "\n",
    "All recurrent layers in PyTorch including RNN, LSTM and GRU, support packed sequences as input, and produce packed\n",
    "output, which can be decoded back to padded tensor .\n",
    "\n",
    "To be able to produce packed sequence, we need to pass length vector to the network, and thus we need a different\n",
    "collate function in dataloader to prepare mini-batches:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def padding_text_length(b):\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1],my_vocab,my_tokenizer) for x in b]\n",
    "    # compute the length of each text sequence and the max length of all text sequence in the mini-batch\n",
    "    len_seq = list(map(len, v))\n",
    "    l = max(len_seq)\n",
    "    return (  # note compare to padding_text function, it returns three tensors, not two.\n",
    "        # 1st output is label tensors\n",
    "        torch.LongTensor([t[0] - 1 for t in b]),\n",
    "        # 2nd output is feature tensors padded with 0.\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t), (0, l - len(t)), mode='constant', value=0) for t in v]),\n",
    "        # 3rd output is length of text sequence tensor\n",
    "        torch.tensor(len_seq)\n",
    "    )\n",
    "\n",
    "\n",
    "train_loader_len = torch.utils.data.DataLoader(train, batch_size=16, collate_fn=padding_text_length, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.1 Rewrite LSTM classifier to accept packed dataloader\n",
    "\n",
    "To use packed dataloader to train the model, we need to modify the LSTMClassifier above. The forward pass will receive\n",
    "both padded mini-batch and the vector of the text sequence lengths. After computing the embedding, we compute\n",
    "packed sequence, pass it to LSTM layer, and then unpack the result back."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class LSTMClassifierWithPackedTensor(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data) - 0.5\n",
    "        self.rnn = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        # encode padded tensor to packed sequence\n",
    "        pad_x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        # pass the packed sequence tensor to lstm layer\n",
    "        pad_x, (h, c) = self.rnn(pad_x)\n",
    "        # decode the packed sequence tensor to padded tensor\n",
    "        # We actually do not return the unpacked result x, because we use output from the hidden layers in the\n",
    "        # following computations. Thus, we can remove the unpacking altogether from this code. The reason we place\n",
    "        # it here is for you to be able to modify this code easily, in case you should need to use network output\n",
    "        # in further computations.\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(pad_x, batch_first=True)\n",
    "        return self.fc(h[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# build a lstm model which accept packed sequence data loader\n",
    "lstm_packed_seq_model = LSTMClassifierWithPackedTensor(my_vocab_size, 64, 32, len(label_classes)).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2 Rewrite the training loop for the new LSTM classifier\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# define a train loop for offset\n",
    "def train_loop_packed_seq(net, dataloader, lr=0.01, optimizer=None, loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                          epoch_size=None,\n",
    "                          report_freq=200, use_pack_sequence=False):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss, acc, count, i = 0, 0, 0, 0\n",
    "    for labels, text, off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels, text = labels.to(device), text.to(device)\n",
    "        if use_pack_sequence:\n",
    "            off = off.to('cpu')\n",
    "        else:\n",
    "            off = off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out, labels)  #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        acc += (predicted == labels).sum()\n",
    "        count += len(labels)\n",
    "        i += 1\n",
    "        if i % report_freq == 0:\n",
    "            print(f\"{count}: acc={acc.item() / count}\")\n",
    "        if epoch_size and count > epoch_size:\n",
    "            break\n",
    "    return total_loss.item() / count, acc.item() / count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.3046875\n",
      "6400: acc=0.3590625\n",
      "9600: acc=0.40875\n",
      "12800: acc=0.457265625\n",
      "16000: acc=0.502625\n",
      "19200: acc=0.5421354166666666\n",
      "22400: acc=0.5751339285714285\n",
      "25600: acc=0.6003125\n",
      "28800: acc=0.6232291666666666\n",
      "32000: acc=0.64346875\n",
      "35200: acc=0.6592045454545454\n",
      "38400: acc=0.6749739583333333\n",
      "41600: acc=0.6879086538461539\n",
      "44800: acc=0.6991294642857143\n",
      "48000: acc=0.7093958333333333\n",
      "51200: acc=0.71890625\n",
      "54400: acc=0.7277573529411765\n",
      "57600: acc=0.7358333333333333\n",
      "60800: acc=0.7423684210526316\n",
      "64000: acc=0.74875\n",
      "67200: acc=0.7546726190476191\n",
      "70400: acc=0.7602130681818182\n",
      "73600: acc=0.7658423913043478\n",
      "76800: acc=0.7706119791666667\n",
      "80000: acc=0.7753375\n",
      "83200: acc=0.7796995192307692\n",
      "86400: acc=0.7841666666666667\n",
      "89600: acc=0.7880022321428571\n",
      "92800: acc=0.7917133620689655\n",
      "96000: acc=0.79528125\n",
      "99200: acc=0.7984879032258064\n",
      "102400: acc=0.80171875\n",
      "105600: acc=0.8043560606060606\n",
      "108800: acc=0.80703125\n",
      "112000: acc=0.8095892857142857\n",
      "115200: acc=0.8122569444444444\n",
      "118400: acc=0.8144679054054054\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.030074466959635417, 0.8154666666666667)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loop_packed_seq(lstm_packed_seq_model, train_loader_len, lr=0.001, use_pack_sequence=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You could notice the accuracy improved a little. Note the structure of the model does not change, we just change the\n",
    "input tensors format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 Bidirectional and multilayer RNNs\n",
    "\n",
    "In above examples, all recurrent networks operated in one direction, from beginning of a sequence to the end. It\n",
    "looks natural, because it resembles the way we read and listen to speech. However, since in many practical cases we\n",
    "have random access to the input sequence, it might make sense to run recurrent computation in both directions. Such\n",
    "networks are called **bidirectional RNNs**, and they can be created by passing **bidirectional=True** parameter to\n",
    "RNN/LSTM/GRU constructor.\n",
    "\n",
    "When dealing with bidirectional network, we would need two hidden state vectors, one for each direction. PyTorch\n",
    "encodes those vectors as one vector of twice larger size, which is quite convenient, because you would normally pass\n",
    "the resulting hidden state to fully-connected linear layer, and you would just need to take this increase in size\n",
    "into account when creating the layer.\n",
    "\n",
    "Recurrent network, one-directional or bidirectional, captures certain patterns within a sequence, and can store them\n",
    "into state vector or pass into output. As with convolutional networks, we can build another recurrent layer on top\n",
    "of the first one to capture higher level patterns, build from low-level patterns extracted by the first layer. This\n",
    "leads us to the notion of **multi-layer RNN**, which consists of two or more recurrent networks, where output of\n",
    "the previous layer is passed to the next layer as input.\n",
    "\n",
    "\n",
    "![multi-layer-lstm](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/multi-layer-lstm.jpg)\n",
    "\n",
    "For more detail of multi layer lstm, please visit https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3\n",
    "\n",
    "PyTorch makes constructing such networks an easy task, because you just need to pass num_layers parameter to \n",
    "RNN/LSTM/GRU constructor to build several layers of recurrence automatically. This would also mean that the size of \n",
    "hidden/state vector would increase proportionally, and you would need to take this into account when handling the \n",
    "output of recurrent layers.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}