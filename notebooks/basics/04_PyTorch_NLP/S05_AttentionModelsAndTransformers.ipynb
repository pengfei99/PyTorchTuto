{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Attention mechanisms and transformers\n",
    "\n",
    "One major drawback of recurrent networks is that all words in a sequence have the same impact on the result. This\n",
    "causes sub-optimal performance with standard LSTM encoder-decoder models for sequence to sequence tasks,\n",
    "such as **Named Entity Recognition** and **Machine Translation**. In reality specific words in the input sequence\n",
    "often have more impact on sequential outputs than others.\n",
    "\n",
    "Consider sequence-to-sequence model, such as machine translation. It is implemented by two recurrent networks,\n",
    "where one network (encoder) would collapse input sequence into hidden state, and another one, decoder, would unroll\n",
    "this hidden state into translated result. The problem with this approach is that final state of the network would\n",
    "have hard time remembering the beginning of a sentence, thus causing poor quality of the model on long sentences.\n",
    "\n",
    "## 5.1 Attention mechanisms\n",
    "Attention Mechanisms provide a means of weighting the contextual impact of each input vector on each output prediction\n",
    "of the RNN. The way it is implemented is by creating shortcuts between intermediate states of the input RNN, and\n",
    "output RNN. In this manner, when generating output symbol **y{t}**, we will take into account all input hidden states\n",
    "**h{i}**, with different weight coefficients **α{t,i}**.\n",
    "![encoder-decoder-attention](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/encoder-decoder-attention.png)\n",
    "\n",
    "You can find more details about the encoder-decoder model with additive attention mechanism. https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
    "\n",
    "### 5.1.1 Attention matrix\n",
    "{αi,j} would represent the degree which certain input words play in generation of a given word in the output sequence.\n",
    "Below is the example of such a matrix:\n",
    "![attention-matrix](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/attention-matrix.png)\n",
    "\n",
    "Read this paper, if you want to know more about attention mechanism. https://arxiv.org/pdf/1409.0473.pdf\n",
    "\n",
    "\n",
    "Attention mechanisms are responsible for much of the current or near current state of the art in Natural language\n",
    "processing. Adding attention however greatly increases the number of model parameters which led to scaling issues\n",
    "with RNNs. A key constraint of scaling RNNs is that the recurrent nature of the models makes it challenging to\n",
    "batch and parallelize training. In an RNN each element of a sequence needs to be processed in sequential order\n",
    "which means it cannot be easily parallelized.\n",
    "\n",
    "Adoption of attention mechanisms combined with this constraint led to the creation of the now **State of the Art\n",
    "Transformer Models** that we know and use today from **BERT** to **OpenGPT3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Transformer models\n",
    "\n",
    "Instead of forwarding the context of each previous prediction into the next evaluation step, transformer models use\n",
    "positional encodings and attention to capture the context of a given input with in a provided window of text.\n",
    "The image below shows how positional encodings with attention can capture context within a given window.\n",
    "\n",
    "![transformer-animated-explanation](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/transformer-animated-explanation.gif)\n",
    "\n",
    "Since each input position is mapped independently to each output position, transformers can parallelize better than\n",
    "RNNs, which enables much larger and more expressive language models. Each attention head can be used to learn\n",
    "different relationships between words that improves downstream Natural Language Processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 BERT (Bidirectional Encoder Representations from Transformers)\n",
    "Bert is a very large multi layer transformer network with **12 layers for BERT-base, and 24 for BERT-large**. The model\n",
    "is first pre-trained on large corpus of text data (WikiPedia + books) using unsupervised training (predicting\n",
    "masked words in a sentence). During pre-training the model absorbs significant level of language understanding\n",
    "which can then be leveraged with other datasets using fine-tuning. This process is called **transfer learning**.\n",
    "\n",
    "![bert-language-modeling-masked-lm](https://raw.githubusercontent.com/pengfei99/PyTorchTuto/main/notebooks/img/bert-language-modeling-masked-lm.png)\n",
    "\n",
    "There are many variations of Transformer architectures including:\n",
    "- BERT\n",
    "- DistilBERT\n",
    "- BigBird\n",
    "- OpenGPT3\n",
    "- ETC.\n",
    "And they all can be fine-tuned. The **HuggingFace**(https://github.com/huggingface/) package provides repository\n",
    "for training many of these architectures with PyTorch.\n",
    "\n",
    "### 5.2.2 Using BERT for text classification\n",
    "Let's see how we can use pre-trained BERT model for solving the sequence classification. We will classify the original\n",
    "AG News dataset.\n",
    "\n",
    "1. load data\n",
    "2. prepare data with bert tokenizer.encode\n",
    "3. load the pre-trained bert model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 1 load news dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: load our dataset:\n",
    "\n",
    "# download data\n",
    "def load_dataset(storage_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root=storage_path)\n",
    "    train_dataset = list(train_dataset)\n",
    "    test_dataset = list(test_dataset)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "path = \"/tmp/pytorch/data\"\n",
    "train, test = load_dataset(path)\n",
    "\n",
    "# build label class list\n",
    "label_classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 2 Prepare data loader with bert tokenizer\n",
    "Because we will be using pre-trained BERT model, we would need to use **specific tokenizer**. First, we will load a\n",
    "tokenizer associated with pre-trained BERT model.\n",
    "\n",
    "HuggingFace library contains a repository of pre-trained models, which you can use just by specifying their names\n",
    "as arguments to from_pretrained functions. All required binary files for the model would automatically be downloaded.\n",
    "\n",
    "However, at certain times you would need to load your own models, in which case you can specify the directory that\n",
    "contains all relevant files, including parameters for tokenizer, config.json file with model parameters,\n",
    "binary weights, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: prepare data loader with bert tokenizer\n",
    "\n",
    "# To load the model from Internet repository using model name.\n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "# bert_model_path = './bert'\n",
    "\n",
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.pad_token)\n",
    "UNK_INDEX = bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokenizer object from transformers lib contains the encode function that can be directly used to encode text:\n",
    "bert_tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's create iterators which we will use during training to access the data. Because BERT uses it's own\n",
    "encoding function, we would need to define a padding function that uses the bert tokenizer to transform text to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# padding function that uses the bert  tokenizer\n",
    "def text_to_tensor_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label,\n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [bert_tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len, v))\n",
    "    return (  # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t), (0, l - len(t)), mode='constant', value=0) for t in v])\n",
    "    )\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=8, collate_fn=text_to_tensor_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=8, collate_fn=text_to_tensor_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 3 load the pre-trained bert model\n",
    "In this example, we will use a pre-trained BERT model called **bert-base-uncased**. Let's load the model using\n",
    "**BertForSequenceClassfication** package. This ensures that our model already has a required architecture for\n",
    "classification, including final classifier. You will see warning message stating that weights of the final\n",
    "classifier are not initialized, and model would require pre-training - that is perfectly okay, because it is\n",
    "exactly what we are about to do!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def select_hardware_for_training(device_name):\n",
    "    if device_name == 'cpu':\n",
    "        return 'cpu'\n",
    "    elif device_name == 'gpu':\n",
    "        return 'cuda' if (device_name == \"\") & torch.cuda.is_available() else 'cpu'\n",
    "    else:\n",
    "        print(\"Unknown device name, choose cpu as default device\")\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = select_hardware_for_training(\"gpu\")\n",
    "\n",
    "bert_model = transformers.BertForSequenceClassification.from_pretrained(bert_model_name, num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 Train the pre-trained bert model\n",
    "Now we are ready to begin training! Because BERT is already pre-trained, we want to start with rather small learning\n",
    "rate in order not to destroy initial weights.\n",
    "\n",
    "All hard work is done by BertForSequenceClassification model. When we call the model on the training data, it\n",
    "returns both loss and network output for input mini_batch. We use loss for parameter optimization\n",
    "(loss.backward() does the backward pass), and out for computing training accuracy by comparing obtained\n",
    "labels labs (computed using argmax) with expected labels.\n",
    "\n",
    "In order to control the process, we accumulate loss and accuracy over several iterations, and print them every\n",
    "report_freq training cycles.\n",
    "\n",
    "This training will likely take quite a long time, so we limit the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define training loop\n",
    "# make iteration larger to train for longer time!\n",
    "def train_loop(model, dataloader, lr=2e-5, optimizer=None, iterations=500,\n",
    "               report_freq=50):\n",
    "    # optimizer can tune model parameter to improve accuracy\n",
    "    optimizer = optimizer or torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    # counter for report activation\n",
    "    i, c = 0, 0\n",
    "    # loss and accuracy stores the model output for each model training step\n",
    "    acc_loss = 0\n",
    "    acc_acc = 0\n",
    "\n",
    "    for labels, texts in dataloader:\n",
    "        labels = labels.to(device) - 1  # get labels in the range 0-3\n",
    "        texts = texts.to(device)\n",
    "        loss, out = model(texts, labels=labels)[:2]\n",
    "        predict_labels = out.argmax(dim=1)\n",
    "        acc = torch.mean((predict_labels == labels).type(torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc_loss += loss\n",
    "        acc_acc += acc\n",
    "        i += 1\n",
    "        c += 1\n",
    "        if i % report_freq == 0:\n",
    "            print(f\"Loss = {acc_loss.item() / c}, Accuracy = {acc_acc.item() / c}\")\n",
    "            c = 0\n",
    "            acc_loss = 0\n",
    "            acc_acc = 0\n",
    "        # we will only learn from 500 text, if you increase iteration number, you can learn from more text sequence\n",
    "        # but it will take more time.\n",
    "        iterations -= 1\n",
    "        if not iterations:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loop(bert_model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can see (especially if you increase the number of iterations and wait long enough) that BERT classification\n",
    "gives us pretty good accuracy! That is because BERT already understands quite well the structure of the language,\n",
    "and we only need to fine-tune final classifier. However, because BERT is a large model, the whole training process\n",
    "takes a long time, and requires serious computational power! (GPU, and preferably more than one).\n",
    "\n",
    "Note: In our example, we have been using one of the smallest pre-trained BERT models. There are larger models that\n",
    "are likely to yield better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 Evaluating the model performance\n",
    "Now we can evaluate performance of our model on test dataset. Evaluation loop is pretty similar to training loop,\n",
    "but we should not forget to switch model to evaluation mode by calling model.eval()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(model, iterations=100):\n",
    "    # set model mode to eval for not changing the weight\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    i = 0\n",
    "    for labels, texts in test_loader:\n",
    "        labels = labels.to(device) - 1\n",
    "        texts = texts.to(device)\n",
    "        _, out = model(texts, labels=labels)[:2]\n",
    "        labs = out.argmax(dim=1)\n",
    "        acc += torch.mean((labs == labels).type(torch.float32))\n",
    "        i += 1\n",
    "        if i > iterations: break\n",
    "    print(f\"Final accuracy: {acc.item() / i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_model(bert_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}